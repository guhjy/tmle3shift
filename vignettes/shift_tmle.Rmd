---
title: "Targeted Learning with Stochastic Treatment Regimes"
author: "[Nima Hejazi](https://nimahejazi.org), [David
  Benkeser](https://www.benkeserstatistics.com/), and [Jeremy
  Coyle](https://github.com/jeremyrcoyle)"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
bibliography: vignette-refs.bib
vignette: >
  %\VignetteIndexEntry{Targeted Learning with Stochastic Treatment Regimes}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, echo=FALSE}
options(scipen=999)
```

## Introduction

Stochastic treatment regimes present a relatively simple manner in which to
assess the effects of continuous treatments by way of parameters that examine
the effects induced by the counterfactual shifting of the observed values of a
treatment of interest. Here, we present an implementation of a new algorithm for
computing targeted minimum loss-based estimates of treatment shift parameters
defined based on a shifting function $d(A,W)$. For a technical presentation of
the algorithm, the interested reader is invited to consult @diaz2018stochastic.
For additional background on Targeted Learning and previous work on stochastic
treatment regimes, please consider consulting @vdl2011targeted,
@vdl2018targeted, and @munoz2012population.

To start, let's load the packages we'll use and set a seed for simulation:

```{r setup}
library(tidyverse)
library(data.table)
library(condensier)
library(sl3)
library(tmle3)
library(tmle3shift)
set.seed(429153)
```

---

## Data and Notation

1. Start with a simple additive shift -- i.e., $d(a,w) = a + \delta$ if
   $a < u(w) - \delta$ or $d(a, w) = a$ if $a \geq u(w) - \delta$.

2. The additive shift will have support everywhere (i.e.,
   $a < u(w)$ is true everywhere).

3. The data structure that we now observe is $O = (W, A, Y)$.

### Simulate Data

```{r}
## simulate simple data for tmle-shift sketch
n_obs <- 1000 # number of observations
n_w <- 1 # number of baseline covariates
tx_mult <- 2 # multiplier for the effect of W = 1 on the treatment

## baseline covariates -- simple, binary
W <- as.numeric(replicate(n_w, rbinom(n_obs, 1, 0.5)))

## create treatment based on baseline W
A <- as.numeric(rnorm(n_obs, mean = tx_mult * W, sd = 1))

## create outcome as a linear function of A, W + white noise
Y <- A + W + rnorm(n_obs, mean = 0, sd = 1)
```

The above composes our observed data structure $O = (W, A, Y)$. To formally
express this fact using the `tlverse` grammar introduced by the `tmle3` package,
we create a single data object and specify the functional relationships between
the nodes in the directed acyclic graph (DAG) via nonparametric structural
equation models (NPSEM), reflected in the node list that we set up:

```{r}
## organize data and nodes for tmle3
data <- data.table(W, A, Y)
node_list <- list(W = "W", A = "A", Y = "Y")
```

We now have an observed data structure (`data`) and a specification of the role
that each variable in the data set plays as the nodes in a DAG.

---

## Methodology

To start, we will initialize a specification for the TMLE of our parameter of
interest (called a `tmle3_Spec` in the `tlverse` nomenclature) simply by calling
`tmle_shift`. We specify the argument `shift_val = 0.5` when initializing the
`tmle3_Spec` object to communicate that we're interested in a shift of $0.5$ on
the scale of the treatment $A$ -- that is, we specify $\delta = 0.5$ (note that
this is an arbitrarily chosen value for this example).

```{r}
# initialize a tmle specification
tmle_spec <- tmle_shift(shift_val = 0.5)
```

As seen above, the `tmle_shift` specification object (like all `tmle3_Spec`
objects) does _not_ store the data for our specific analysis of interest.
Instead, we use `tmle3_Task` objects to store such data. The initialized
`tmle_shift` specification object makes it simple to generate a task, which can
be done simply by calling the `make_tmle_task` method (and passing in the data
and node list we built above):

```{r}
# define data as task object
tmle_task <- tmle_spec$make_tmle_task(data, node_list)
tmle_task
```

Prior to discussing how to fit a TML estimator, we'll first discuss how to
incorporate ensemble learning into the necessary nuisance regressions using the
`sl3` package.

### _Interlude:_ Constructing Optimal Stacked Regressions with `sl3`

To easily incorporate ensemble machine learning into the estimation procedure,
we rely on the facilities provided in the [`sl3` R
package](https://sl3.tlverse.org). For a complete guide on using the `sl3` R
package, consider consulting https://sl3.tlverse.org, or https://tlverse.org for
the `tlverse` ecosystem, of which `sl3` is a major part.

Using the framework provided by the [`sl3` package](https://sl3.tlverse.org),
the nuisance parameters of the TML estimator may be fit with ensemble learning,
using the cross-validation framework of the Super Learner algorithm of
@vdl2007super.

```{r}
# learners used for conditional expectation regression (e.g., outcome)
lrn1 <- Lrnr_mean$new()
lrn2 <- Lrnr_glm$new()
lrn3 <- Lrnr_ranger$new()
sl_lrn <- Lrnr_sl$new(
  learners = list(lrn1, lrn2, lrn3),
  metalearner = Lrnr_nnls$new()
)

# learners used for conditional density regression (e.g., propensity score)
lrn1_dens <- Lrnr_condensier$new(
  nbins = 25, bin_estimator = lrn1,
  bin_method = "dhist"
)
lrn2_dens <- Lrnr_condensier$new(
  nbins = 20, bin_estimator = lrn2,
  bin_method = "dhist"
)
lrn3_dens <- Lrnr_condensier$new(
  nbins = 15, bin_estimator = lrn3,
  bin_method = "dhist"
)
sl_lrn_dens <- Lrnr_sl$new(
  learners = list(lrn1_dens, lrn2_dens, lrn3_dens),
  metalearner = Lrnr_solnp_density$new()
)
```

As seen above, we can generate two different ensemble learners for the two
nuisance regressions that must be fit in the process of computing this TML
estimator. In particular, we use a Super Learner composed of an intercept model,
a GLM, and a random forest (as implemented in `ranger`) for fitting the outcome
regressions (often denoted "Q" in the literature) while we use variations of
these learners for the conditional density estimation needed in fitting the
treatment mechanism (often denoted "g" in the literature).

We make the above explicit with respect to standard notation by bundling the
ensemble learners into a list object below:

```{r}
# specify outcome and treatment regressions and create learner list
Q_learner <- sl_lrn
g_learner <- sl_lrn_dens
learner_list <- list(Y = Q_learner, A = g_learner)
```

The `learner_list` object above specifies the role that each of the ensemble
learners we've generated is to play in computing initial estimators to be used
in building a TMLE for the parameter of interest here. In particular, it makes
explicit the fact that our `Q_learner` is used in fitting the outcome regression
while our `g_learner` is used in fitting our treatment mechanism regression.

### Targeted Estimation Stochastic Interventions Effects with Ensemble Learning

Having defined and initialized ensemble learners to be used in computing our
TML estimator, we now return to the process of computing this estimator using
the framework provided in the `tmle3` package and extended to this stochastic
intervention target parameter in this package.

In fact, we need only call a handful of functions to compute our TML estimator,
each of which encapsulate one of the major steps of the targeted maximum
likelihood estimation methodology.

First, we use the `tmle_spec` object for our treatment shift parameter in order
to define an _initial likelihood_ for the statistical estimation problem. This
_initial likelihood_ requires only the specification of a `tmle_task` object,
which contains the data, and a `learner_list`, which specifies the learning
algorithms (ensemble learners, in our example) to be used in each of the initial
estimators (of relevant factors of the likelihood) that must be computed. To do
this easily, we need only invoke the `make_initial_likelihood` method:

```{r}
# define likelihood
likelihood_init <- tmle_spec$make_initial_likelihood(tmle_task, learner_list)
```

The key steps of computing a TML estimator are components needed to update the
initial estimators in a manner that is optimal in the sense of loss-based
minimization. To do this, our next step is to initialize an _update method_,
which we do by simply invoking the `make_updater` method of our `tmle_spec`
object. With the _update method_ and _initial likelihood_ in hand, we combine
these to initialize a _targeted likelihood_ object, which concerns itself only
with updating _factors_ of the likelihood relevant for optimizing the TML
estimate of the parameter -- below, we do this by invoking the `new` method of
the `Targeted_Likelihood` object that is included in the core `tmle3` package:

```{r}
# define update method (submodel and loss function)
updater <- tmle_spec$make_updater()
likelihood_targeted <- Targeted_Likelihood$new(likelihood_init, updater)
```

Next, we combine our targeted likelihood and `tmle_task` in order to specify the
arguments relevant for computing our TML estimator. As this is a bookkeeping
step, we need only notice that we make use of the `make_params` method that is
specific to the `tmle_spec` of our shifted treatment parameter and provide the
resultant object (`tmle_params`) to the `updater` object we initialized above.

```{r}
# define parameter
tmle_params <- tmle_spec$make_params(tmle_task, likelihood_targeted)
updater$tmle_params <- tmle_params
```

Finally, having put together the various ingredients, we are ready to compute
our TML estimator, which we do simply through invoking the `fit_tmle3` function
of the `tmle3` package. Note that all information specific to estimating our
parameter of interest are contained within the latter three arguments to this
function:

```{r}
# fit tmle update
tmle_fit <- fit_tmle3(tmle_task, likelihood_targeted, tmle_params, updater)
tmle_fit
```

The `print` method of the resultant `tmle_fit` object conveniently displays the
results from computing our TML estimator.

### Statistical Inference for Targeted Maximum Likelihood Estimates

Recall that the asymptotic distribution of TML estimators has been studied
thoroughly:
$$\psi_n - \psi_0 = (P_n - P_0) \cdot D(\bar{Q}_n^*, g_n) + R(\hat{P}^*, P_0),$$
which, provided the following two conditions:

1. If $D(\bar{Q}_n^*, g_n)$ converges to $D(P_0)$ in $L_2(P_0)$ norm, and
2. the size of the class of functions considered for estimation of $\bar{Q}_n^*$
   and $g_n$ is bounded (technically, $\exists \mathcal{F}$ st
   $D(\bar{Q}_n^*, g_n) \in \mathcal{F}$ *__whp__*, where $\mathcal{F}$ is a
   Donsker class),
readily admits the conclusion that
$\psi_n - \psi_0 = (P_n - P_0) \cdot D(P_0) + R(\hat{P}^*, P_0)$.

Under the additional condition that the remainder term $R(\hat{P}^*, P_0)$
decays as $o_P \left( \frac{1}{\sqrt{n}} \right),$ we have that
$$\psi_n - \psi_0 = (P_n - P_0) \cdot D(P_0) + o_P \left( \frac{1}{\sqrt{n}}
 \right),$$
which, by a central limit theorem, establishes a Gaussian limiting distribution
for the estimator:

$$\sqrt{n}(\psi_n - \psi) \to N(0, V(D(P_0))),$$
where $V(D(P_0))$ is the variance of the efficient influence curve (canonical
gradient) when $\psi$ admits an asymptotically linear representation.

The above implies that $\psi_n$ is a $\sqrt{n}$-consistent estimator of $\psi$,
that it is asymptotically normal (as given above), and that it is locally
efficient. This allows us to build Wald-type confidence intervals in a
straightforward manner:

$$\psi_n \pm z_{\alpha} \cdot \frac{\sigma_n}{\sqrt{n}},$$
where $\sigma_n^2$ is an estimator of $V(D(P_0))$. The estimator $\sigma_n^2$
may be obtained using the bootstrap or computed directly via the following

$$\sigma_n^2 = \frac{1}{n} \sum_{i = 1}^{n} D^2(\bar{Q}_n^*, g_n)(O_i)$$

Having now re-examined these facts, let's simply examine the results of
computing our TML estimator:

```{r}
tmle_fit
```

## References

